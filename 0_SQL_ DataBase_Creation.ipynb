{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T13:42:17.354247Z",
     "start_time": "2020-07-10T13:42:17.349258Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We convert all xlsx to csv files and we save them in a specific path. We replace all DataStream errors \"$$ER\" to NaN values, to do this we use <a href=\"https://www.w3schools.com/python/python_regex.asp\"> regex expressions </a>. The follow link explain how to <a href=\"https://www.youtube.com/watch?v=4IhfbxhoAn8\"> create a server in your own computer.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:47:39.489935Z",
     "start_time": "2021-04-03T21:47:39.464942Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "credentials = ast.literal_eval(open(r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Python\\credentials.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T14:24:40.293107Z",
     "start_time": "2020-07-12T14:24:39.796189Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "First we transform the files that all the data is in an unique excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:57:37.621047Z",
     "start_time": "2021-04-03T21:56:29.945748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all .xlsx files in the path\n",
    "os.chdir(r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\2. Listado\")\n",
    "files_xlsx=[]\n",
    "# We create a list with the files that finish in *.xlsx\n",
    "for file in glob.glob(\"*.xlsx\"):\n",
    "    files_xlsx.append(file)\n",
    "# We read the file with pandas and we save it as csv\n",
    "for file in files_xlsx:\n",
    "    df_excel=pd.read_excel(r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\2. Listado\" + \"\\\\\" + file)\n",
    "    # before save excel as csv we transform the datastream errors \"$$ER\" to NaN values -- we use regex expresions\n",
    "    df_excel= df_excel.replace(r'^..ER: ', np.nan, regex=True)    \n",
    "    name = file.replace(\"xlsx\", \"csv\")\n",
    "    df_excel.to_csv(r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\3. CSV--Database\" + \"/\" + name,\n",
    "                   index =False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T15:16:09.536699Z",
     "start_time": "2020-07-10T15:14:01.796025Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We merge with pandas the Excel files when the information is in various files. We transform the files into csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T16:57:26.483809Z",
     "start_time": "2021-04-03T15:37:26.636057Z"
    }
   },
   "outputs": [],
   "source": [
    "# The paths of the files that the data is in several documents \n",
    "paths=[r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\2. Listado\\esg scores\"\n",
    "      , r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\2. Listado\\market based\"]\n",
    "names =[\"monthly_esg.csv\",\"daily_market.csv\"] #name of the new document\n",
    "\n",
    "# We merge each excel file that are in the paths of the list  \"pahts\"\n",
    "for path, name in zip (paths, names):\n",
    "    os.chdir(path)\n",
    "    files_xlsx=[]\n",
    "    df_excel = pd.DataFrame() \n",
    "    \n",
    "    # list with the files to merge\n",
    "    for file in glob.glob(\"*.xlsx\"):\n",
    "        files_xlsx.append(file)    \n",
    "    #we merge the excel files\n",
    "    for file in files_xlsx:\n",
    "        df_excel = df_excel.append(pd.read_excel(path + \"/\" + file), ignore_index=True)\n",
    "    \n",
    "    # before save excel as csv we transform the datastream errors \"$$ER\" to NaN values -- we use regex expresions\n",
    "    df_excel= df_excel.replace(r'^..ER: ', np.nan, regex=True)\n",
    "    # we save the csv\n",
    "    df_excel.to_csv(r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\3. CSV--Database\\%s\" % name,\n",
    "                   index =False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T13:42:17.354247Z",
     "start_time": "2020-07-10T13:42:17.349258Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "The packages that we need to create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T18:50:16.749910Z",
     "start_time": "2021-04-03T18:50:16.458780Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2 \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We create a dictionary with the name of the files as key, the first list is the name of the columns we are interesting in, the second list will be the names of the columns in our SQL database, the third list will be the primary keys of the table in the database. The fourth item is the format of the columns (string, float, integer). The last item indicates the date format. \n",
    "Also we make the connection to the data base with the <code>engine</code> instruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:50:38.380190Z",
     "start_time": "2021-04-03T21:50:38.367194Z"
    }
   },
   "outputs": [],
   "source": [
    "# engine = create_engine('postgresql://postgres:postgres@localhost/')\n",
    "# engine = create_engine('postgresql+psycopg2://postgres:postgres@localhost/paper_2')\n",
    "# engine = create_engine('postgresql+psycopg2://vilicas@finance-project:Palamones1996*@finance-project.postgres.database.azure.com/azure_finance')\n",
    "engine = create_engine(credentials['sqlalchemy'][0]+ 'ESGRefinitiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:57:37.636043Z",
     "start_time": "2021-04-03T21:57:37.623034Z"
    }
   },
   "outputs": [],
   "source": [
    "files={\n",
    "'c_iso_country_currency' : [[ 'Alphabetic Code', 'Alpha-3 code', 'Currency', 'ENTITY' ],\n",
    "[ 'currency_iso_id', 'country_iso_id', 'currency_name', 'region_name' ],\n",
    "[ 'currency_iso_id', 'country_iso_id' ],\n",
    "{ 'Alphabetic Code':'string', 'Alpha-3 code':'string', 'Currency':'string', 'ENTITY':'string' },\n",
    "[ ]],\n",
    "'classification_iso_country' : [[ 'Alpha-3 code', 'Numeric', 'English short name', 'Alpha-2 code', 'Continent' ],\n",
    "[ 'country_iso_id', 'country_numeric_code', 'country_name', 'county_2', 'continent' ],\n",
    "[ 'country_iso_id' ],\n",
    "{ 'Alpha-3 code':'string', 'Numeric':'float64', 'English short name':'string', 'Alpha-2 code':'string', 'Continent':'string' },\n",
    "[ ]],\n",
    "'classification_icb_sector' : [[ 'FTAG5', 'ICBSUN', 'FTAG4', 'ICBSN', 'FTAG3', 'ICBSSN', 'FTAG2', 'ICBIN' ],\n",
    "[ 'subsector_id', 'subsector_name', 'sector_code', 'sector_name', 'supersector_code', 'supersector_name', 'industry_code', 'industry_name' ],\n",
    "[ 'subsector_id' ],\n",
    "{ 'FTAG5':'float64', 'ICBSUN':'string', 'FTAG4':'float64', 'ICBSN':'string', 'FTAG3':'float64', 'ICBSSN':'string', 'FTAG2':'float64', 'ICBIN':'string' },\n",
    "[ ]],\n",
    "'classification_thomson_sector' : [[ 'TR4', 'TR4N', 'TR3', 'TR3N', 'TR2', 'TR2N', 'TR1', 'TR1N' ],\n",
    "[ 'industry_id', 'industry_name', 'industry_group_code', 'industry_group_name', 'business_code', 'business_name', 'economic_code', 'economic_name' ],\n",
    "[ 'industry_id' ],\n",
    "{ 'TR4':'float64', 'TR4N':'string', 'TR3':'float64', 'TR3N':'string', 'TR2':'float64', 'TR2N':'string', 'TR1':'float64', 'TR1N':'string' },\n",
    "[ ]],\n",
    "'classification_ds_country' : [[ 'GEOG', 'INISO', 'GEOGC', 'GEOGN' ],\n",
    "[ 'country_ds_id', 'country_iso_id', 'country_alpha_code', 'country_name' ],\n",
    "[ 'country_ds_id' ],\n",
    "{ 'GEOG':'float64', 'INISO':'string', 'GEOGC':'string', 'GEOGN':'string' },\n",
    "[ ]],\n",
    "'company_country' : [[ 'DSCD', 'CODOM', 'GEOG', 'COINM', 'GEOL', 'WC06027', 'WC06026', 'WC06025' ],\n",
    "[ 'datastream_id', 'ds_country_domicile_id', 'ds_country_group_id', 'ds_country_incorporation_id', 'ds_county_listing_id', 'wc_county_domicile', 'wc_country_offices', 'wc_postcode_offices' ],\n",
    "[ 'datastream_id' ],\n",
    "{ 'DSCD':'string', 'CODOM':'float64', 'GEOG':'float64', 'COINM':'float64', 'GEOL':'float64', 'WC06027':'string', 'WC06026':'string', 'WC06025':'string' },\n",
    "[ ]],\n",
    "'companies_esg_refinitiv' : [[ 'internal_id', 'DSCD', 'TR4', 'FTAG5', 'RIC', 'WC06035', 'ISIN', 'TYPE', 'WC06001', 'BDATE', 'TIME', 'NAME', 'MNEM', 'WC05601' ],\n",
    "[ 'internal_id', 'datastream_id', 'thomson_industry_id', 'icb_subsector_id', 'ric_code', 'worldscope_code', 'isin_code', 'instrument_type', 'company_name', 'ds_first_value', 'ds_last_value', 'ds_name', 'mnemonic', 'ticker' ],\n",
    "[ 'datastream_id' ],\n",
    "{ 'internal_id':'string', 'DSCD':'string', 'TR4':'float64', 'FTAG5':'float64', 'RIC':'string', 'WC06035':'string', 'ISIN':'string', 'TYPE':'string', 'WC06001':'string', 'NAME':'string', 'MNEM':'string', 'WC05601':'string' },\n",
    "[  'BDATE', 'TIME' ]],\n",
    "'yearly_accounting' : [[ 'Date', 'Code', 'WC06099', 'WC05350', 'WC08326', 'WC08301', 'WC02999', 'WC03351', 'WC03501', 'WC03251', 'WC01001', 'WC07011', 'WC02201', 'WC03040', 'WC02051', 'WC18199', 'WC04601', 'WC02501', 'WC01201' ],\n",
    "[ 'yearly_date', 'datastream_id', 'document_currency_iso_id', 'end_fiscal_period', 'return_on_assets', 'return_on_equity', 'total_assets', 'total_liabilities', 'common_equity', 'long_term_debt', 'net_sales', 'number_of_employees', 'current_assets', 'accounts_payable', 'receivables', 'net_debt', 'additions_fixed_assets', 'net_property_plant_equipment', 'research_development_expense' ],\n",
    "[ 'yearly_date', 'datastream_id' ],\n",
    "{ 'Code':'string', 'WC06099':'string', 'WC08326':'float64', 'WC08301':'float64', 'WC02999':'float64', 'WC03351':'float64', 'WC03501':'float64', 'WC03251':'float64', 'WC01001':'float64', 'WC07011':'float64', 'WC02201':'float64', 'WC03040':'float64', 'WC02051':'float64', 'WC18199':'float64', 'WC04601':'float64', 'WC02501':'float64', 'WC01201':'float64' },\n",
    "[  'Date', 'WC05350' ]],\n",
    "'monthly_esg' : [[ 'Date', 'Code', 'TRESGS', 'TRESGCS', 'TRESGCCS', 'ENSCORE', 'CGSCORE', 'SOSCORE' ],\n",
    "[ 'monthly_date', 'datastream_id', 'esg_score', 'combined_score', 'controversies_score', 'environment_pillar_score', 'governance_pillar_score', 'social_pillar_score' ],\n",
    "[ 'monthly_date', 'datastream_id' ],\n",
    "{ 'Code':'string', 'TRESGS':'float64', 'TRESGCS':'float64', 'TRESGCCS':'float64', 'ENSCORE':'float64', 'CGSCORE':'float64', 'SOSCORE':'float64' },\n",
    "[  'Date' ]],\n",
    "'daily_market' : [[ 'Date', 'Code', 'P', 'MV', 'X(P)~U$', 'X(MV)~U$', 'PI', 'RZ', 'RI', 'SPLDTE', 'SPLFCT' ],\n",
    "[ 'daily_date', 'datastream_id', 'close_price', 'market_value', 'close_price_usd', 'market_value_usd', 'price_index', 'return_index_paid', 'total_return_index', 'split_date', 'split_factor' ],\n",
    "[ 'daily_date', 'datastream_id' ],\n",
    "{ 'Code':'string', 'P':'float64', 'MV':'float64', 'X(P)~U$':'float64', 'X(MV)~U$':'float64', 'PI':'float64', 'RZ':'float64', 'RI':'float64' },\n",
    "[  'Date', 'SPLDTE', 'SPLFCT' ]],\n",
    "'classification_iso_currency' : [[ 'Alphabetic Code', 'Currency', 'Status', 'Numeric Code', 'Minor unit', 'SICUR', 'MNEM', 'NAME' ],\n",
    "[ 'currency_iso_id', 'currency_name', 'status', 'digit_code', 'minor_unit', 'ds_currency_symbol_id', 'ds_currency_serie_usd_id', 'ds_currency_serie_name' ],\n",
    "[ 'currency_iso_id' ],\n",
    "{ 'Alphabetic Code':'string', 'Currency':'string', 'Status':'string', 'Numeric Code':'float64', 'Minor unit':'float64', 'SICUR':'string', 'MNEM':'string', 'NAME':'string' },\n",
    "[ ]],\n",
    "'exchange_rates' : [[ 'Date', 'Code', 'ER' ],\n",
    "[ 'daily_date_id', 'ds_currency_serie_id', 'exchange_rate' ],\n",
    "[ 'daily_date_id', 'ds_currency_serie_id' ],\n",
    "{ 'Code':'string', 'ER':'float64' },\n",
    "[  'Date' ]],\n",
    "'economic_series' : [[ 'UNDP Country', 'Year', 'Alpha-3 code', 'IDH', 'GDP percapita usd constant' ],\n",
    "[ 'onu_country', 'yearly_date', 'country_iso_id', 'idh', 'pib_per_usd' ],\n",
    "[ 'onu_country', 'yearly_date' ],\n",
    "{ 'UNDP Country':'string', 'Alpha-3 code':'string', 'IDH':'float64', 'GDP percapita usd constant':'float64' },\n",
    "[  'Year' ]],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T12:19:17.635715Z",
     "start_time": "2020-07-10T12:19:17.623746Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We use <b>psycopg2</b> adapter to send SQL instructions to our database from python, we make the connection to the database and we open the cursor to perform SQL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:57:37.714004Z",
     "start_time": "2021-04-03T21:57:37.639028Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=\"ESGRefinitiv\", user=credentials['psycopg2'][0], \n",
    "                        password=credentials['psycopg2'][1], host=credentials['psycopg2'][2], port =credentials['psycopg2'][3])\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T12:34:47.651676Z",
     "start_time": "2020-07-10T12:34:47.641704Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We introduce our files into SQL database as tables. The name of the the tables is the same that the names of the files. If there exist a table with the same name in our database we delete this table in CASCADE way. The importance of using <code>method=\"multi\"</code> in this <a href=\"https://stackoverflow.com/questions/29706278/python-pandas-to-sql-with-sqlalchemy-how-to-speed-up-exporting-to-ms-sql\">StackOverflow answer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:58:21.471308Z",
     "start_time": "2021-04-03T21:57:37.715005Z"
    }
   },
   "outputs": [],
   "source": [
    "for file_name in files:\n",
    "    tables_names =[] #We create a list with the names of our SQL tables\n",
    "    #If the table exist we delete it\n",
    "    cur.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS %s CASCADE\"\"\" % (file_name)) \n",
    "    conn.commit()\n",
    "    # We read the files and we export them to the database\n",
    "    ruta= r\"C:\\Users\\pvilas\\OneDrive - unizar.es\\Tesis\\Artículo 2 ISP\\1. Data\\3. CSV--Database\" +\"\\\\\" + file_name +\".csv\"\n",
    "    # Name of the columns, format of the columns and dates\n",
    "    table = pd.read_csv(ruta, usecols= files[file_name][0], dtype=files[file_name][3] ,parse_dates=files[file_name][4])\n",
    "    # Change the order of the columns\n",
    "    table = table[files[file_name][0]]\n",
    "    # Change the name of the columns\n",
    "    table.columns= files[file_name][1]\n",
    "    #Drop NA values in the index\n",
    "    table= table.dropna(subset=files[file_name][2])\n",
    "    # Table to SQL --pandas dataframe to sql\n",
    "    table.to_sql(file_name, engine, if_exists = \"replace\", index=False, chunksize=150000, method=\"multi\") \n",
    "    # Names of each table in the database\n",
    "    tables_names.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T12:34:47.651676Z",
     "start_time": "2020-07-10T12:34:47.641704Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We create the <b>primary keys</b> of our tables. Remember the third list in each dictionary key contains the primary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:58:22.102828Z",
     "start_time": "2021-04-03T21:58:21.472290Z"
    }
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    cur.execute(\"\"\"\n",
    "    ALTER TABLE %s ADD PRIMARY KEY (%s) \"\"\" % (file,\",\".join(files[file][2]))) \n",
    "    #ALTER TABLE table_name ADD PRIMARY KEY (key_id)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T20:25:04.521261Z",
     "start_time": "2020-07-10T20:25:04.515277Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "The are some Foreign key that do not have the unique restriction. We create this restriction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T20:58:38.900177Z",
     "start_time": "2021-04-03T20:58:38.866189Z"
    }
   },
   "outputs": [],
   "source": [
    "# We send the instruction to ensures that all values in a column are different in this way we will be able to create a fk  \n",
    "cur.execute(\"\"\"ALTER TABLE \"classification_iso_currency\"\n",
    "   ADD CONSTRAINT \"ds_currency_serieusd_id\" UNIQUE (\"ds_currency_serie_usd_id\");\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T12:19:17.635715Z",
     "start_time": "2020-07-10T12:19:17.623746Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We create the foreign keys of our tables. The dictionary key is the name of the foreign key, the first value in the list is the table name of the foreign key, the second value is the column name, the third value is the name of the table of reference and the last value is the name of the column in the table of reference. <br>\n",
    "A FOREIGN KEY is a field (or collection of fields) in one table that refers to the PRIMARY KEY/UNIQUE column in another table. The table containing the foreign key is called the child table, and the table containing the candidate key is called the referenced or parent table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:58:22.133780Z",
     "start_time": "2021-04-03T21:58:22.106790Z"
    }
   },
   "outputs": [],
   "source": [
    "fk_dict={\n",
    "'fk1' : [ 'companies_esg_refinitiv', 'icb_subsector_id', 'classification_icb_sector', 'subsector_id', ], \n",
    "'fk2' : [ 'companies_esg_refinitiv', 'thomson_industry_id', 'classification_thomson_sector', 'industry_id', ],  \n",
    "'fk4' : [ 'company_country', 'ds_country_group_id', 'classification_ds_country', 'country_ds_id', ],  \n",
    "'fk6' : [ 'company_country', 'ds_county_listing_id', 'classification_ds_country', 'country_ds_id', ], \n",
    "'fk7' : [ 'classification_ds_country', 'country_iso_id', 'classification_iso_country', 'country_iso_id', ], \n",
    "'fk8' : [ 'yearly_accounting', 'document_currency_iso_id', 'classification_iso_currency', 'currency_iso_id', ], \n",
    "'fk9' : [ 'exchange_rates', 'ds_currency_serie_id', 'classification_iso_currency', 'ds_currency_serie_usd_id', ], \n",
    "'fk10' : [ 'c_iso_country_currency', 'currency_iso_id', 'classification_iso_currency', 'currency_iso_id', ], \n",
    "'fk11' : [ 'c_iso_country_currency', 'country_iso_id', 'classification_iso_country', 'country_iso_id', ], \n",
    "'fk12' : [ 'yearly_accounting', 'datastream_id', 'companies_esg_refinitiv', 'datastream_id', ], \n",
    "'fk13' : [ 'monthly_esg', 'datastream_id', 'companies_esg_refinitiv', 'datastream_id', ], \n",
    "'fk14' : [ 'daily_market', 'datastream_id', 'companies_esg_refinitiv', 'datastream_id', ], \n",
    "'fk15' : [ 'company_country', 'datastream_id', 'companies_esg_refinitiv', 'datastream_id', ], \n",
    "'fk16' : [ 'economic_series', 'country_iso_id', 'classification_iso_country', 'country_iso_id', ], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T20:24:39.949323Z",
     "start_time": "2020-07-12T20:24:39.942374Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    "We send the instructon to create the foreign key constraint of our database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:58:29.795481Z",
     "start_time": "2021-04-03T21:58:22.138778Z"
    }
   },
   "outputs": [],
   "source": [
    "for fk in fk_dict:    \n",
    "    cur.execute(\"\"\"\n",
    "       ALTER TABLE %s ADD CONSTRAINT %s FOREIGN KEY (%s)\n",
    "       REFERENCES %s (%s);\"\"\"  % (fk_dict[fk][0], fk, fk_dict[fk][1], fk_dict[fk][2], fk_dict[fk][3]))\n",
    "    \n",
    "    # ALTER TABLE \"company_general_information\" ADD CONSTRAINT \"fk1\" FOREIGN KEY (\"datastream_id\")\n",
    "    # REFERENCES \"monthly_esg_scores\" (\"datastream_id\");\"\"\" )\n",
    "    \n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T14:40:55.714978Z",
     "start_time": "2020-07-12T14:24:39.837Z"
    }
   },
   "source": [
    "<body> <p  style = \"font-family:georgia,garamond,serif;font-size:16px;text-align:justify\"></body>\n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T19:01:28.147236Z",
     "start_time": "2020-07-22T18:45:27.011Z"
    }
   },
   "outputs": [],
   "source": [
    "os.system('shutdown -s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T21:53:18.858010Z",
     "start_time": "2021-04-03T21:53:18.851990Z"
    }
   },
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
